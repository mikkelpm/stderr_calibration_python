{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f76aab30",
   "metadata": {},
   "source": [
    "# Standard errors for calibrated parameters: Neoclassical Growth Model example\n",
    "\n",
    "*We are grateful to Ben Moll for suggesting this example. Any errors are our own.*\n",
    "\n",
    "\n",
    "In this notebook we will work through the basic logic of [Cocci & Plagborg-MÃ¸ller (2021)](https://scholar.princeton.edu/mikkelpm/calibration) in the context of calibrating a simple version of the Neoclassical Growth Model (NGM). Though the model is highly stylized, it helps provide intuition for our procedures. Please see our paper for other, more realistic empirical applications.\n",
    "\n",
    "\n",
    "## Model\n",
    "\n",
    "We consider the simplest version of the NGM without population growth or technological growth. As explained in [Dirk Krueger's lecture notes (section 5.4)](https://perhuaman.files.wordpress.com/2014/06/macrotheory-dirk-krueger.pdf), this model implies three steady state equations:\n",
    "1. **Euler equation:** $\\frac{1}{1+r} = \\beta$, where $\\beta$ is the household discount factor and $r$ is the real interest rate.\n",
    "2. **Capital accumulation:** $\\frac{I}{K} = \\delta$, where $\\delta$ is the depreciation rate and $I/K$ is the ratio of investment to capital stock.\n",
    "3. **Rental rate of capital:** $\\frac{K}{Y} = \\frac{\\alpha}{\\frac{1}{\\beta}-1+\\delta}$, where $\\alpha$ is the capital elasticity in the production function and $\\frac{K}{Y}$ is the capital-output ratio.\n",
    "\n",
    "We want to use these equations to calibrate (i.e., estimate) $\\beta$, $\\delta$, and $\\alpha$.\n",
    "\n",
    "\n",
    "## Estimation\n",
    "\n",
    "We can measure the steady-state values of the variables on the left-hand side of the above equations by computing sample averages of the relevant time series over a long time span. Denote these sample averages by $\\widehat{\\frac{1}{1+r}}$, $\\widehat{\\frac{I}{K}}$, and $\\widehat{\\frac{K}{Y}}$, respectively. We can then obtain natural method-of-moment estimates of the three parameters as follows:\n",
    "$$\\hat{\\beta} = \\widehat{\\frac{1}{1+r}},\\quad \\hat{\\delta}=\\widehat{\\frac{I}{K}},\\quad \\hat{\\alpha}=\\widehat{\\frac{K}{Y}}\\left(\\frac{1}{\\hat{\\beta}}-1+\\hat{\\delta} \\right).$$\n",
    "\n",
    "\n",
    "## Standard errors\n",
    "\n",
    "The sample averages above are subject to statistical noise due to the finite time sample. This statistical noise obviously carries over to the estimated parameters. To gauge the extent of the noise, we seek to compute standard errors for the estimated parameters.\n",
    "\n",
    "The key ingredients into calculating standard errors for the parameters are the standard errors for the three sample averages. Denote these by $\\hat{\\sigma}\\left(\\widehat{\\frac{1}{1+r}}\\right)$, $\\hat{\\sigma}\\left(\\widehat{\\frac{I}{K}}\\right)$, and $\\hat{\\sigma}\\left(\\widehat{\\frac{K}{Y}}\\right)$. To compute these standard errors, one would have to apply a formula that accounts for serial correlation in the data, such as the [Newey-West estimator](https://www.stata.com/manuals16/tsnewey.pdf). Let's assume that we have already done this.\n",
    "\n",
    "It's immediate what the standard errors for $\\hat{\\beta}$ and $\\hat{\\delta}$ are: They simply equal $\\hat{\\sigma}\\left(\\widehat{\\frac{1}{1+r}}\\right)$ and $\\hat{\\sigma}\\left(\\widehat{\\frac{I}{K}}\\right)$, respectively. However, the standard error for $\\hat{\\alpha}$ is not so obvious, as this estimator depends implicitly on all three sample averages:\n",
    "$$\\hat{\\alpha}=\\widehat{\\frac{K}{Y}}\\left(\\frac{1}{\\widehat{\\frac{1}{1+r}}}-1+\\widehat{\\frac{I}{K}} \\right) \\approx \\alpha + x_1\\left(\\widehat{\\frac{1}{1+r}}-\\frac{1}{1+r}\\right) + x_2\\left(\\widehat{\\frac{I}{K}}-\\frac{I}{K}\\right) + x_3\\left(\\widehat{\\frac{K}{Y}}-\\frac{K}{Y}\\right),$$\n",
    "where the last approximation is a [delta method](https://en.wikipedia.org/wiki/Delta_method) linearization with\n",
    "$$x_1=-\\frac{\\frac{K}{Y}}{\\left(\\frac{1}{1+r}\\right)^2},\\quad x_2=\\frac{K}{Y},\\quad x_3=\\left(\\frac{1}{\\frac{1}{1+r}}-1+\\frac{I}{K} \\right).$$\n",
    "Thus, to compute the standard error for $\\hat{\\alpha}$, we need to know not just the standard errors for the individual sample averages, but also their correlations.\n",
    "\n",
    "\n",
    "## Limited-information inference\n",
    "\n",
    "If we observed annual data on the real interest rate, capital, investment, and output, it would not be too difficult to estimate the correlations of the sample moments. This could again be done using a Newey-West long-run variance estimator. Yet, in practice we may face several potential complicating factors:\n",
    "1. **Non-overlapping samples:** Perhaps we do not observe all time series over the same time span. This makes it difficult to apply the usual Newey-West procedure.\n",
    "2. **Different data frequencies:** Perhaps the real interest rate series is obtained from daily yields on inflation-protected bonds, while the other time series are annual. This again complicates the econometric analysis.\n",
    "3. **Finite-sample accuracy:** The Newey-West procedure is known to suffer from small-sample biases when the data exhibits strong time series dependence. Trying to exploit estimates of the correlations of the sample averages could therefore lead to distorted inference in realistic sample sizes.\n",
    "4. **Non-public data:** Perhaps some of the sample averages were not computed by ourselves, but only obtained from other papers (say, a paper that imputes real interest rates by feeding bond yields through a structural model). Those other papers may report the standard errors for their respective individual moments, but not the correlations with other moments that we rely on.\n",
    "\n",
    "A pragmatic limited-information approach would therefore be to give up on computing the precise standard error of $\\hat{\\alpha}$ and instead compute an *upper bound* on it. We seek an upper bound that depends only on the standard errors of the individual moments, not their correlations.\n",
    "\n",
    "The key to obtaining such a bound is the following inequality for random variables $X$ and $Y$:\n",
    "$$\\text{Std}(X+Y) = \\sqrt{\\text{Var}(X+Y)} = \\sqrt{\\text{Var}(X) + \\text{Var}(Y)+2\\text{Corr}(X,Y)\\text{Std}(X)\\text{Std}(Y)} \\leq \\sqrt{\\text{Var}(X) + \\text{Var}(Y)+2\\text{Std}(X)\\text{Std}(Y)} = \\sqrt{(\\text{Std}(X)+\\text{Std}(Y))^2} = \\text{Std}(X)+\\text{Std}(Y).$$\n",
    "Applying this logic to the earlier approximation for $\\hat{\\alpha}$, we get the bound (up to a small approximation error)\n",
    "$$\\text{Std}(\\hat{\\alpha}) \\leq |x_1|\\text{Std}\\left(\\widehat{\\frac{1}{1+r}}\\right) + |x_2|\\text{Std}\\left(\\widehat{\\frac{I}{K}}\\right) + |x_3|\\text{Std}\\left(\\widehat{\\frac{K}{Y}}\\right).$$\n",
    "We can therefore compute an upper bound for the standard error of $\\hat{\\alpha}$ as follows:\n",
    "$$\\hat{\\sigma}(\\hat{\\alpha}) \\leq |\\hat{x}_1|\\hat{\\sigma}\\left(\\widehat{\\frac{1}{1+r}}\\right) + |\\hat{x}_2|\\hat{\\sigma}\\left(\\widehat{\\frac{I}{K}}\\right) + |\\hat{x}_3|\\hat{\\sigma}\\left(\\widehat{\\frac{K}{Y}}\\right),$$\n",
    "where\n",
    "$$\\hat{x}_1=-\\frac{\\widehat{\\frac{K}{Y}}}{\\left(\\widehat{\\frac{1}{1+r}}\\right)^2},\\quad \\hat{x}_2=\\widehat{\\frac{K}{Y}},\\quad \\hat{x}_3=\\left(\\frac{1}{\\widehat{\\frac{1}{1+r}}}-1+\\widehat{\\frac{I}{K}} \\right).$$\n",
    "Notice that this upper bound only depends on things that we know: the sample averages themselves and their individual standard errors (but not the correlations across moments).\n",
    "\n",
    "It is not possible to improve the bound without further knowledge of the correlation structure: The bound turns out to equal the actual standard error when the three sample averages are perfectly correlated with each other (either positively or negatively, depending on the signs of $x_1,x_2,x_3$). This is proved in Lemma 1 in [our paper](https://scholar.princeton.edu/mikkelpm/calibration). For this reason, we refer to the standard error bound as the *worst-case standard error*.\n",
    "\n",
    "\n",
    "## Numerical example\n",
    "\n",
    "Our software package allows for easy calculation of worst-case standard errors. As an illustration, suppose the sample averages (with standard errors in parenthesis) equal\n",
    "$$\\widehat{\\frac{1}{1+r}}=0.98\\;(0.02), \\quad \\widehat{\\frac{I}{K}}=0.08\\;(0.005), \\quad \\widehat{\\frac{K}{Y}} = 3\\;(0.04).$$\n",
    "We define the model equations and data as follows. Let $\\theta=(\\beta,\\delta,\\alpha)$ and $\\mu=(\\frac{1}{1+r},\\frac{I}{K},\\frac{K}{Y})$ denote the vectors of parameters and moments, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcaefc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from stderr_calibration import MinDist # Minimum distance routines\n",
    "\n",
    "# Define mapping from parameters to moments\n",
    "h = lambda theta: np.array([theta[0],theta[1],theta[2]/(1/theta[0]-1+theta[1])])\n",
    "\n",
    "# Define empirical moments and their s.e.\n",
    "mu = np.array([0.98,0.08,3])\n",
    "sigma = np.array([0.02,0.005,0.04])\n",
    "\n",
    "# Define MinDist object used in later analysis\n",
    "obj = MinDist(h,mu,moment_se=sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74317c8",
   "metadata": {},
   "source": [
    "We can now estimate the parameters and compute their worst-case standard errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862ac1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_estim = np.array([mu[0],mu[1],mu[2]*(1/mu[0]-1+mu[1])]) # Use closed-form formula for estimates\n",
    "res = obj.fit(param_estim=param_estim,eff=False)\n",
    "print('Parameter estimates')\n",
    "print(res['estim'])\n",
    "print('Worst-case standard errors')\n",
    "print(res['estim_se'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c35f87",
   "metadata": {},
   "source": [
    "## Over-identification test\n",
    "\n",
    "The textbook NGM also implies that the steady-state labor share of income should equal $1-\\alpha$. Suppose we measure the sample average of the labor share to be 0.65 with a standard error of 0.03. We wish to test the over-identifying restriction that the earlier estimate of $\\hat{\\alpha}$ is consistent with this moment. We can do this as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8ff590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define expanded mapping that also includes over-identifying moment\n",
    "h_expand = lambda theta: np.append(h(theta),1-theta[2])\n",
    "\n",
    "# Define expanded empirical moments and standard errors\n",
    "mu_expand = np.append(mu,0.65)\n",
    "sigma_expand = np.append(sigma,0.03)\n",
    "\n",
    "# Define new MinDist object and fit\n",
    "obj_expand = MinDist(h_expand,mu_expand,moment_se=sigma_expand)\n",
    "res_expand = obj_expand.fit(param_estim=param_estim,eff=False) # Same parameter estimates as before\n",
    "\n",
    "# Over-identification test\n",
    "res_overid = obj_expand.overid(res_expand)\n",
    "print('Error in matching non-targeted moment')\n",
    "print(res_overid['moment_error'][3]) # The non-targeted moment is the fourth one\n",
    "print('Standard error')\n",
    "print(res_overid['moment_error_se'][3])\n",
    "print('t-statistic')\n",
    "print(res_overid['tstat'][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d42fb64",
   "metadata": {},
   "source": [
    "Since the t-statistic exceeds 1.96 in absolute value, we can reject the validity of the model at the 95% level.\n",
    "\n",
    "\n",
    "## Other features in the paper\n",
    "\n",
    "The above NGM example is very simple and stylized. In [our paper](https://scholar.princeton.edu/mikkelpm/calibration) we extend the basic ideas along various dimensions that are relevant for applied research. For example:\n",
    "- The matched moments need not be simple sample averages, but could be regression coefficients, quantiles, etc. The moments need not be related to steady-state quantities, but could involve essentially any feature of the available data.\n",
    "- The calibration (method-of-moments) estimator need not be available in closed form (usually one would obtain it by numerical optimization).\n",
    "- If some, but not all, of the correlations between the empirical moments are known, this can be exploited to sharpen inference.\n",
    "- If we are interested in a function of the model parameters (such as a counterfactual quantity) rather than the parameters *per se*, we can compute worst-case standard errors for that function, too.\n",
    "- If we are interested in testing several parameter restrictions at once, a joint test is available that has valid size asymptotically.\n",
    "- All computational routines can handle models with a relatively large number of parameters and moments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

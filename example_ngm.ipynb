{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f76aab30",
   "metadata": {},
   "source": [
    "# Standard errors for calibrated parameters: Neoclassical Growth Model example\n",
    "\n",
    "*We are grateful to Ben Moll for suggesting this example. Any errors are our own.*\n",
    "\n",
    "\n",
    "In this notebook we will work through the basic logic of [Cocci & Plagborg-MÃ¸ller (2023)](https://arxiv.org/abs/2109.08109) in the context of calibrating a simple version of the Neoclassical Growth Model (NGM). Though the model is highly stylized, it helps provide intuition for our procedures. Please see our paper for other, arguably more realistic, empirical applications.\n",
    "\n",
    "\n",
    "## Model\n",
    "\n",
    "We consider the simplest version of the NGM without population growth or technological growth. As explained in section 3.4 of [Dirk Krueger's lecture notes](https://perhuaman.files.wordpress.com/2014/06/macrotheory-dirk-krueger.pdf) (note the difference in notation), this model implies three key steady-state equations:\n",
    "1. **Euler equation:** $r = \\rho$, where $\\rho$ is the household discount rate and $r$ is the real interest rate.\n",
    "2. **Capital accumulation:** $\\frac{I}{K} = \\delta$, where $\\delta$ is the capital depreciation rate and $I/K$ is the ratio of investment to capital stock.\n",
    "3. **Rental rate of capital:** $\\frac{K}{Y} = \\frac{\\alpha}{\\rho+\\delta}$, where $\\alpha$ is the capital elasticity in the production function and $\\frac{K}{Y}$ is the capital-output ratio.\n",
    "\n",
    "We want to use these equations to calibrate (i.e., estimate) the parameters $\\rho$, $\\delta$, and $\\alpha$.\n",
    "\n",
    "\n",
    "## Estimation\n",
    "\n",
    "We can measure the steady-state values of the variables on the left-hand side of the above equations by computing sample averages of the relevant time series over a long time span. Denote these sample averages by $\\hat{r}$, $\\widehat{\\frac{I}{K}}$, and $\\widehat{\\frac{K}{Y}}$, respectively. We can then obtain natural method-of-moment estimates of the three parameters as follows:\n",
    "$$\\hat{\\rho} = \\hat{r},\\quad \\hat{\\delta}=\\widehat{\\frac{I}{K}},\\quad \\hat{\\alpha}=\\widehat{\\frac{K}{Y}}\\left(\\hat{\\rho}+\\hat{\\delta} \\right).$$\n",
    "\n",
    "\n",
    "## Standard errors\n",
    "\n",
    "The sample averages above are subject to statistical noise due to the finite time sample. This statistical noise obviously carries over to the estimated parameters. To gauge the extent of the noise, we seek to compute standard errors for the estimated parameters.\n",
    "\n",
    "The key ingredients into calculating standard errors for the parameters are the standard errors for the three sample averages. Denote these by $\\hat{\\sigma}\\left(\\hat{r}\\right)$, $\\hat{\\sigma}\\left(\\widehat{\\frac{I}{K}}\\right)$, and $\\hat{\\sigma}\\left(\\widehat{\\frac{K}{Y}}\\right)$. To compute these standard errors, one would have to apply a formula that accounts for serial correlation in the data, such as the [Newey-West long-run variance estimator](https://www.stata.com/manuals16/tsnewey.pdf). Let's assume that we have already done this.\n",
    "\n",
    "It's immediate what the standard errors for $\\hat{\\rho}$ and $\\hat{\\delta}$ are: They simply equal $\\hat{\\sigma}\\left(\\hat{r}\\right)$ and $\\hat{\\sigma}\\left(\\widehat{\\frac{I}{K}}\\right)$, respectively. However, the standard error for $\\hat{\\alpha}$ is not so obvious, as this estimator depends implicitly on all three sample averages:\n",
    "$$\\hat{\\alpha}=\\widehat{\\frac{K}{Y}}\\left(\\hat{r}+\\widehat{\\frac{I}{K}}\\right) \\approx \\alpha + x_1\\left(\\hat{r}-r\\right) + x_2\\left(\\widehat{\\frac{I}{K}}-\\frac{I}{K}\\right) + x_3\\left(\\widehat{\\frac{K}{Y}}-\\frac{K}{Y}\\right),$$\n",
    "where the last approximation is a [delta method](https://en.wikipedia.org/wiki/Delta_method) linearization with\n",
    "$$x_1=x_2=\\frac{K}{Y},\\quad x_3=r+\\frac{I}{K}.$$\n",
    "Since $\\hat{\\alpha}$ is approximately a linear combination of several sample averages, computing its standard error requires not just the standard errors for the individual sample averages, but also their correlations.\n",
    "\n",
    "\n",
    "## Limited-information inference\n",
    "\n",
    "If we observed annual data on the real interest rate, capital, investment, and output, it would not be too difficult to estimate the cross-correlations of the sample averages. This could again be done using the Newey-West estimator of the $3 \\times 3$ long-run variance-covariance matrix. Yet, in practice we may face several potential complicating factors:\n",
    "1. **Non-overlapping samples:** Perhaps we do not observe all time series over the same time span. This makes it difficult to apply the usual Newey-West formulas.\n",
    "2. **Different data frequencies:** Perhaps the real interest rate series is obtained from daily yields on inflation-protected bonds, while the other time series are annual. This again complicates the econometric analysis.\n",
    "3. **Finite-sample accuracy:** The Newey-West procedure is known to suffer from small-sample biases when the data exhibits strong time series dependence. Trying to exploit estimates of the correlations of the sample averages could therefore lead to distorted inference in realistic sample sizes.\n",
    "4. **Non-public data:** Perhaps some of the sample averages were not computed by ourselves, but only obtained from other papers (say, a paper that imputes real interest rates by feeding bond yields through a structural model). Those other papers may report the standard errors for their respective individual moments, but not the correlations with other moments that our calibration relies on.\n",
    "\n",
    "A pragmatic limited-information approach would therefore be to give up on computing the precise standard error of $\\hat{\\alpha}$ and instead compute an *upper bound* on it. We seek an upper bound that depends only on the standard errors of the individual moments, not their correlations.\n",
    "\n",
    "The key to obtaining such a bound is the following inequality for random variables $X$ and $Y$:\n",
    "$$\\begin{align*}\n",
    "\\text{Std}(X+Y) &= \\sqrt{\\text{Var}(X+Y)} \\\\\n",
    "&= \\sqrt{\\text{Var}(X) + \\text{Var}(Y)+2\\text{Corr}(X,Y)\\text{Std}(X)\\text{Std}(Y)} \\\\\n",
    "&\\leq \\sqrt{\\text{Var}(X) + \\text{Var}(Y)+2\\text{Std}(X)\\text{Std}(Y)} \\\\\n",
    "&= \\sqrt{(\\text{Std}(X)+\\text{Std}(Y))^2} \\\\\n",
    "&= \\text{Std}(X)+\\text{Std}(Y).\n",
    "\\end{align*}$$\n",
    "Applying this logic to the earlier approximation for $\\hat{\\alpha}$, we get the bound (up to a small approximation error)\n",
    "$$\\text{Std}(\\hat{\\alpha}) \\leq |x_1|\\text{Std}\\left(\\hat{r}\\right) + |x_2|\\text{Std}\\left(\\widehat{\\frac{I}{K}}\\right) + |x_3|\\text{Std}\\left(\\widehat{\\frac{K}{Y}}\\right).$$\n",
    "We can therefore compute an upper bound for the standard error of $\\hat{\\alpha}$ as follows:\n",
    "$$\\hat{\\sigma}(\\hat{\\alpha}) \\leq |\\hat{x}_1|\\hat{\\sigma}\\left(\\hat{r}\\right) + |\\hat{x}_2|\\hat{\\sigma}\\left(\\widehat{\\frac{I}{K}}\\right) + |\\hat{x}_3|\\hat{\\sigma}\\left(\\widehat{\\frac{K}{Y}}\\right),$$\n",
    "where\n",
    "$$\\hat{x}_1=\\hat{x}_2=\\widehat{\\frac{K}{Y}},\\quad \\hat{x}_3=\\left(\\hat{r}+\\widehat{\\frac{I}{K}} \\right).$$\n",
    "Notice that this upper bound only depends on things that we know: the sample averages themselves and their individual standard errors (but not the correlations across moments).\n",
    "\n",
    "It's impossible to improve the bound without further knowledge of the correlation structure: The bound turns out to equal the actual standard error when the three sample averages are perfectly correlated with each other. This is proved in Lemma 1 in [our paper](https://arxiv.org/abs/2109.08109). For this reason, we refer to the standard error bound as the *worst-case standard error*.\n",
    "\n",
    "\n",
    "## Numerical example\n",
    "\n",
    "Our software package makes it easy to calculate worst-case standard errors. As an illustration, suppose the sample averages (with standard errors in parentheses) equal\n",
    "$$\\hat{r}=0.02\\;(0.002), \\quad \\widehat{\\frac{I}{K}}=0.08\\;(0.01), \\quad \\widehat{\\frac{K}{Y}} = 3\\;(0.1).$$\n",
    "We define the model equations and data as follows. Let $\\theta=(\\rho,\\delta,\\alpha)$ and $\\mu=(r,\\frac{I}{K},\\frac{K}{Y})$ denote the vectors of parameters and moments, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcaefc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from stderr_calibration import MinDist # Minimum distance routines\n",
    "\n",
    "# Define mapping from parameters to moments\n",
    "h = lambda theta: np.array([theta[0],theta[1],theta[2]/(theta[0]+theta[1])])\n",
    "\n",
    "# Define empirical moments and their s.e.\n",
    "mu = np.array([0.02,0.08,3])\n",
    "sigma = np.array([0.002,0.01,0.1])\n",
    "\n",
    "# Define MinDist object used in later analysis\n",
    "obj = MinDist(h,mu,moment_se=sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74317c8",
   "metadata": {},
   "source": [
    "We can now estimate the parameters and compute their worst-case standard errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862ac1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_estim = np.array([mu[0],mu[1],mu[2]*(mu[0]+mu[1])]) # Closed-form formula for estimates\n",
    "res = obj.fit(param_estim=param_estim,eff=False)\n",
    "print('Parameter estimates')\n",
    "print(res['estim'])\n",
    "print('Worst-case standard errors')\n",
    "print(res['estim_se'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c35f87",
   "metadata": {},
   "source": [
    "(Note: The derivatives required to compute $\\hat{x}_1,\\hat{x}_2,\\hat{x}_3$ are produced under the hood by the software using finite differences. We could have also computed the parameter estimates $\\hat{\\rho},\\hat{\\delta},\\hat{\\alpha}$ numerically if we didn't have a closed-form formula. See our [other example](https://mikkelpm.github.io/stderr_calibration_python/example.html) for details.)\n",
    "\n",
    "\n",
    "## Over-identification test\n",
    "\n",
    "The textbook NGM also implies that the steady-state labor share of income should equal $1-\\alpha$. Suppose we measure the sample average of the labor share to be 0.6 with a standard error of 0.01. We wish to test the over-identifying restriction that the earlier estimate of $\\hat{\\alpha}$ is consistent with this moment. We can do this as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8ff590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define expanded mapping that also includes over-identifying moment\n",
    "h_expand = lambda theta: np.append(h(theta),1-theta[2])\n",
    "\n",
    "# Define expanded empirical moments and standard errors\n",
    "mu_expand = np.append(mu,0.6)\n",
    "sigma_expand = np.append(sigma,0.01)\n",
    "\n",
    "# Define new MinDist object and fit\n",
    "obj_expand = MinDist(h_expand,mu_expand,moment_se=sigma_expand)\n",
    "res_expand = obj_expand.fit(param_estim=param_estim,weight_mat=np.diag(np.array([1,1,1,0])),eff=False)\n",
    "# Same parameter estimates as before (weight matrix indicates that these estimates do not use the fourth moment)\n",
    "\n",
    "# Over-identification test\n",
    "res_overid = obj_expand.overid(res_expand)\n",
    "print('Error in matching non-targeted moment')\n",
    "print(res_overid['moment_error'][3]) # The non-targeted moment is the fourth one\n",
    "print('Standard error')\n",
    "print(res_overid['moment_error_se'][3])\n",
    "print('t-statistic')\n",
    "print(res_overid['tstat'][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d42fb64",
   "metadata": {},
   "source": [
    "Since the absolute value of the t-statistic lies between 1.64 and 1.96, we can reject the validity of the model at the 10% significance level, but not at the 5% level.\n",
    "\n",
    "The over-identification test checks how different the estimate $\\hat{\\alpha}$ would have been if we had instead computed it as the sample average capital share of income $1-0.6=0.4$. Is the difference in parameter estimates between the two calibration strategies too large to be explained by statistical noise?\n",
    "\n",
    "\n",
    "## Other features in the paper\n",
    "\n",
    "The above NGM example is very simple and stylized. In [our paper](https://arxiv.org/abs/2109.08109) we extend the basic ideas along various dimensions that are relevant for applied research. For example:\n",
    "- The matched moments need not be simple sample averages, but could be regression coefficients, quantiles, etc. The moments need not be related to steady-state quantities, but could involve essentially any feature of the available data.\n",
    "- The calibration (method-of-moments) estimator need not be available in closed form (usually one would obtain it by numerical optimization).\n",
    "- If some, but not all, of the correlations between the empirical moments are known, this can be exploited to sharpen inference.\n",
    "- If we have more moments to match than parameters to estimate, we can compute the optimal weighting of the moments that minimizes the worst-case standard errors of the parameters.\n",
    "- If we are interested in a function of the model parameters (such as a counterfactual quantity) rather than the parameters *per se*, we can compute worst-case standard errors for that function, too.\n",
    "- If we are interested in testing several parameter restrictions at once, a joint test is available that has valid size asymptotically.\n",
    "- All computational routines can handle models with relatively large numbers of parameters and moments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
